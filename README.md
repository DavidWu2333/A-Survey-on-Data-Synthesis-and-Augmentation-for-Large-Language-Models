# A Review for Data Synthesis and Augmentation for Large Models

This is a repository for ...


## Table of Contents
* [Types of Data Synthesis and Augmentation](#types-of-data-synthesis-and-augmentation)
* [Applications](#applications)
  * [Data preparation](#data-preparation)
  * [Pretraining](#pretraining)
  * [Prompt](#prompt)
  * [Fine-Tuning](#fine-tuning)
  * [Alignment with Human Preferences](#alignment-with-human-preferences)
  * [Knowledge Base](#knowledge-base)
  * [RAG and Other Tools](#rag-and-other-tools)
  * [Evaluation](#evaluation)
  * [Optimization and Deployment](#optimization-and-deployment)
  * [Applications](#applications-1)
  * [Agent](#agent)
* [Dataset](#dataset)
  * [Dataset Examples](#dataset-examples)

# Types of Data Synthesis and Augmentation
| Paper                                             |  Published in | Code/Project|                                  
|---------------------------------------------------|:-------------:|:------------:|
[example paper](https://arxiv.org/abs/...)|arxuv 2023|-|

# Applications
## Data preparation
| Paper                                             |  Published in | Code/Project|                                  
|---------------------------------------------------|:-------------:|:------------:|

## Pretraining
| Paper                                             |  Published in | Code/Project|                                  
|---------------------------------------------------|:-------------:|:------------:|
[TinyStories: How small can language models be and still speak coherent English](https://arxiv.org/abs/2305.07759)|arxiv 2023|https://huggingface.co/roneneldan|
[Textbooks are all you need](https://arxiv.org/abs/2306.11644)|arxiv 2023|-|
[Textbooks are all you need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)|arxiv 2023|-|

## Fine-Tuning
| Paper                                             |  Published in | Code/Project|                                  
|---------------------------------------------------|:-------------:|:------------:|
[Large language models can self-improve](https://arxiv.org/abs/2210.11610)|arxiv 2022|-|
[STaR: Bootstrapping reasoning with reasoning](https://arxiv.org/abs/2203.14465)|arxiv 2022|-|
[Language models can teach themselves to program better](https://arxiv.org/abs/2207.14502)|arxiv 2022|https://github.com/microsoft/PythonProgrammingPuzzles|
[Self-Instruct: Aligning language models with self-generated instructions](https://arxiv.org/abs/2212.10560)|arxiv 2023|https://github.com/yizhongw/self-instruct|
[Stanford Alpaca: An Instruction-following LLaMA Model](https://github.com/tatsu-lab/stanford_alpaca)|github 2023|https://github.com/tatsu-lab/stanford_alpaca|
[Code alpaca: An instruction-following llama model for code generation](https://github.com/sahil280114/codealpaca)|github 2023|https://github.com/sahil280114/codealpaca|
[Code Llama: Open foundation models for code](https://arxiv.org/abs/2308.12950)|arxiv 2023|https://github.com/meta-llama/codellama|
[WizardLM: Empowering large language models to follow complex instructions](https://arxiv.org/abs/2304.12244)|arxiv 2023|https://github.com/nlpxucan/WizardLM|
[WizardCode: Empowering code large language models with Evol-Instruct](https://arxiv.org/abs/2306.08568)|arxiv 2023|https://github.com/nlpxucan/WizardLM|
[WizardMath: Empowering mathematical reasoning for large language models via reinforced evol-instruct](https://arxiv.org/abs/2308.09583)|arxiv 2023|https://github.com/nlpxucan/WizardLM|
[Magicoder: Source code is all you need](https://arxiv.org/abs/2312.02120)|arxiv 2023|https://github.com/ise-uiuc/magicoder|
[MetaMeth: Bootstap your own mathematical questions for large language models](https://arxiv.org/abs/2309.12284)|arxiv 2024|https://meta-math.github.io/|
[DeepSeek-Prover: Advancing theorem proving in LLMs through large-scale synthetic data](https://arxiv.org/abs/2405.14333v1)|arxiv 2024|-|
[Conmmon 7B language models already possess strong math capabilities](https://arxiv.org/abs/2403.04706)|arxiv 2024|https://github.com/Xwin-LM/Xwin-LM|
[Augmenting math word problems via iterative question composing](https://arxiv.org/abs/2401.09003)|arxiv 2024|https://huggingface.co/datasets/Vivacem/MMIQC|


## Prompt
| Paper                                             |  Published in | Code/Project|                                  
|---------------------------------------------------|:-------------:|:------------:|


## Alignment with Human Preferences
| Paper                                             |  Published in | Code/Project|                                  
|---------------------------------------------------|:-------------:|:------------:|


## Knowledge Base
| Paper                                             |  Published in | Code/Project|                                  
|---------------------------------------------------|:-------------:|:------------:|


 
## RAG and Other Tools
| Paper                                             |  Published in | Code/Project|                                  
|---------------------------------------------------|:-------------:|:------------:|


## Evaluation
| Paper                                             |  Published in | Code/Project|                                  
|---------------------------------------------------|:-------------:|:------------:|


## Optimization and Deployment
| Paper                                             |  Published in | Code/Project|                                  
|---------------------------------------------------|:-------------:|:------------:|


## Applications
| Paper                                             |  Published in | Code/Project|                                  
|---------------------------------------------------|:-------------:|:------------:|



## Agent
| Paper                                             |  Published in | Code/Project|                                  
|---------------------------------------------------|:-------------:|:------------:|


# Dataset
## Dataset Examples
| Dataset                                            |  Home/Github | Download link|                                  
|---------------------------------------------------|:-------------:|:------------:|
